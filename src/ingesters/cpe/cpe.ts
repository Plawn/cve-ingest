import zlib from "https://deno.land/std@0.177.0/node/zlib.ts";
import { Readable } from "https://deno.land/std@0.177.0/node/stream.ts";
import { CPEFeed, CPEFeedType, RawCPE } from "./../../api/feed.ts";
import { SimpleBatcher } from "./../../utils/batcher.ts";
import { sha1Hex } from "./../../utils/format.ts";
import { parser, pick, streamArray, tarGzStreamArrayExtractor } from "./../../utils/streaming.ts";
import { Persistor } from "./../../utils/persistor.ts";
import { CPE } from "./type.ts";
import { PoolExecutor, sleep } from "../../utils/pool.ts";

/**
 * Taken from cve-search
 * @param cpe
 * @returns
 */
function generateTitle(cpe: string) {
  let title = "";

  const cpeSplit = cpe.split(":");
  // # Do a very basic test to see if the CPE is valid
  if (cpeSplit.length === 13) {
    // # Combine vendor, product and version
    title = cpeSplit.slice(3, 6).join(" ");

    // # If "other" is specified, add it to the title
    if (cpeSplit[12] != "*") {
      title += cpeSplit[12];
    }

    // # Capitilize each word
    // title = title.title()

    // # If the target_sw is defined, add "for <target_sw>" to title
    if (cpeSplit[10] != "*") {
      title += " for " + cpeSplit[10];
    }

    // # In CPE 2.3 spaces are replaced with underscores. Undo it
    title = title.replace("_", " ");

    // # Special characters are escaped with \. Undo it
    title = title.replace("\\", "");
  }

  return title;
}

/**
 * This function enables us to stream download the archive, while reading the content, while parsing the content, while pushing the result to the persistor
 *
 * This enables us to limit the memory consumption and do all the work in memory as a stream
 * This is the most efficient way to do it
 * @param stream
 * @param onItem
 * @returns
 */
async function executePipeline(
  stream: Readable,
  onItem: (item: RawCPE) => Promise<void>,
  pool: PoolExecutor,
) {
  // const pipeline = stream // here we have a stream of bytes
  //   .pipe(zlib.createGunzip()) // take the tar.gz archive and stream the content of the archive
  //   .pipe(parser()) // stream the bytes of the first file into a parser
  //   .pipe(pick({ filter: "matches" })) // takes the entry called "matches"
  //   .pipe(streamArray()); // takes the entries of "matches" and stream each token

  const pipeline = tarGzStreamArrayExtractor<CPEFeedType>(stream, 'matches');

  let counter = 0;
  pipeline.on("data", ({ value }: { value: RawCPE }) => { // here we are called with each RawCPE item
    onItem(value).then(() => {
      ++counter;
      // console.log(counter);
      if (counter % 1000 === 0) {
        console.log(counter);
      }
    });
  });
  // pipeline.read
  // just iterate
  const it = pipeline[Symbol.asyncIterator]();
  // for await (const value of pipeline) {
  //   // onItem(value).then(() => {
  //   //   ++counter;
  //   //   // console.log(counter);
  //     if (counter % 1000 === 0) {
  //       console.log(counter);
  //       await sleep(10);
  //     }
  //   // });
  //   // with this way to iterate we don't let a chance to push to db before calling pool.isReady
  //   if (counter % 5000 === 0) {
  //     console.log(counter);
  //   }
  //   await pool.isReady(); // poors man's backpressure
  // }

  return new Promise<void>((resolve) => {
    pipeline.on("end", () => {
      // console.log('end');
      resolve();
    });
  });
}

function makeCpeAndVersion(item: RawCPE): [CPE, string] {
  const uri = item.cpe23Uri;
  const cpe: CPE = {
    title: generateTitle(uri),
    cpe_2_2: uri,
    cpe_name: item["cpe_name"],
    vendor: uri.split(":")[3],
    product: uri.split(":")[4],
  };

  // could use a reduce if we want to get fancy
  let version_info = "";
  if ("versionStartExcluding" in item) {
    cpe.versionStartExcluding = item["versionStartExcluding"];
    version_info += cpe.versionStartExcluding + "_VSE";
  }

  if ("versionStartIncluding" in item) {
    cpe.versionStartIncluding = item["versionStartIncluding"];
    version_info += cpe.versionStartIncluding + "_VSI";
  }

  if ("versionEndExcluding" in item) {
    cpe.versionEndExcluding = item["versionEndExcluding"];
    version_info += cpe.versionEndExcluding + "_VEE";
  }

  if ("versionEndIncluding" in item) {
    cpe.versionEndIncluding = item["versionEndIncluding"];
    version_info += cpe.versionEndIncluding + "_VEI";
  }

  return [cpe, version_info];
}

function prepareCpeItem(item: RawCPE) {
  return new Promise<CPE>((resolve) => {
    const [cpe, version_info] = makeCpeAndVersion(item);
    const sha1Hash = sha1Hex(cpe.cpe_2_2 + version_info);
    cpe.id = sha1Hash;
    return resolve(cpe);
  });
}

export class CPEIngester {
  protected batcher: SimpleBatcher<CPE>;
  protected persistor: Persistor<CPE, { id: string }>;
  protected pool: PoolExecutor;
  constructor(
    persistor: Persistor<CPE, { id: string }>,
  ) {
    this.pool = new PoolExecutor(5);
    this.batcher = new SimpleBatcher<CPE>(5000, this.flusher, this.pool);
    this.persistor = persistor;
  }
  protected upsertData = async (stream: Readable) => {
    await executePipeline(
      stream,
      (item: RawCPE) => {
        return prepareCpeItem(item).then(this.batcher.enqueue);
      },
      this.pool,
    );
  };

  /**
   * Called when a batcher is flushed with the batch being flushed
   * @param batch
   * @returns
   */
  protected flusher = async (batch: CPE[]) => {
    const size = batch.length;
    if (size === 0) {
      return;
    }
    console.log('doing batch');
    const start = new Date().getTime();
    const batchInsertResult = await this.persistor.persistMany(batch);

    const end = new Date().getTime();
    console.log("batch insert took", end - start, "for", batchInsertResult);
  };

  public async update() {
    const f = new CPEFeed();
    const lastUpdate = new Date(); // TODO: remove stub and fetch from DB
    if (await f.isUpToDate(lastUpdate)) {
      console.log("not up to date, will download");
      const result = await f.get();
      await this.upsertData(result);
      console.log("download done");
    }
    console.log('awaiting last bacthes');
    await this.batcher.flush();
  }
}
