import { Batcher, SimpleBatcher } from "./../../utils/batcher.ts";
import { prepareDate, formatDate } from "./../../utils/format.ts";
import { Persistor } from "./../../utils/persistor.ts";
import { CPE } from "./type.ts";
import { Counter, PoolExecutor, sleep, toMs } from "../../utils/pool.ts";
import { CpeChunk, CPENVD, Update, CPESearch, CPENVDResult } from "../../utils/types.ts";
import { getDateChunks } from "../../utils/utils.ts";
import { UpdateProps } from "../cve/type.ts";
import { ProgressHandler } from "../../utils/progress.ts";
import apikey from "../../../../apikey.json" assert { type: "json" };

// The value is based on the maximum possible for the API given by NVD
const pageSize = 10000;

async function getCpesBetweenPaged(
  page: number,
  pageSize: number,
  initialPersistance: boolean,
  start?: Date,
  end?: Date,
): Promise<CPENVDResult> {
  let url = "";
  const startIndex = page * pageSize;
  if (!initialPersistance && start && end) {

    console.log("Test pour avoir les dates correctes")
    console.log(start);
    const startString = prepareDate(start);
    const endString = prepareDate(end);
    url = `https://services.nvd.nist.gov/rest/json/cpes/2.0?lastModStartDate=${startString}&lastModEndDate=${endString}&startIndex=${startIndex}`;
    console.log(url);
  } else {
    url = `https://services.nvd.nist.gov/rest/json/cpes/2.0?startIndex=${startIndex}`;
  }
  for (; ;) {
    try {
      const res = await fetch(url, { method: 'GET', headers: apikey});
      await sleep(toMs(600));
      const text = await res.text();
      if (res.status != 200) {
        console.log(`warn: got status ${res.statusText}`);
        await sleep(toMs(6_000));
      } else {
        try {
          const value = JSON.parse(text);
          console.log(`could get good answer for index ${startIndex}`)
          return value;
        } catch (_e) {
          console.error(res.status, text);
          // throw e;
        }
      }
    } catch (e) {
      console.error(" --------------------------------------------------------------------------\n--------------------------------------------------------------------------\nError fetching", e);
    }
  }
}

// in Days
const timeChukSize = 30;


async function* getCpesBetween(
  initialPersistance: boolean,
  baseStart?: Date,
  baseEnd?: Date,
): AsyncGenerator<CpeChunk> {
  if (!initialPersistance && baseStart && baseEnd) {
    for (const [start, end] of getDateChunks(baseStart, baseEnd, timeChukSize)) {  
      console.log(`Doing CPEs between ${start} and ${end}`);
      let hasRemaining = true;
      let page = 0;
      
      while (hasRemaining) {
        const result = await getCpesBetweenPaged(page, pageSize, initialPersistance, start, end);
        // if the result is exactly the size of the page then we waste one query
        hasRemaining = (result.products.length >= pageSize);
        const expectedPages = result.totalResults / pageSize;
        page++;
        yield { result, page, expectedPages, start, end, hasRemaining };
      }
    }
  } else {
    console.log("Doing all the CPEs existing");
    let hasRemaining = true;
    let page = 0;
    
    while (hasRemaining) {
      const result = await getCpesBetweenPaged(page, pageSize, initialPersistance);
      // if the result is exactly the size of the page then we waste one query
      hasRemaining = (result.products.length >= pageSize);
      const expectedPages = result.totalResults / pageSize;
      page++;
      yield { result, page, expectedPages, hasRemaining };
    }
  }
}



async function doOnePage(
  chunk: CpeChunk,
  counter: Counter,
  batcher: Batcher<unknown>,
) {
  const {
    result,
    page,
    expectedPages
  } = chunk;
  console.log("doing page", page, "of expected pages", expectedPages);
  const start = new Date().getTime();
  const re = result.products
    .map((c) =>
      handleOne(c.cpe)
        .then((e) => {
          // We give the cpe to the batcher so that it can be persisted
          batcher.enqueue(e);
          counter.increment();
        })
    );
  await Promise.all(re);
  const end = new Date().getTime();
  const took = (end - start) / 1000;
  console.log(`page ${page} took ${took} s`);
}

// Gets the information from the item and outputs the cpe with the right format to be handled by the persistor
async function handleOne(
  item: CPENVD
  ): Promise<CPESearch> {
  const cpe = {
    id: item.cpeNameId,
    created_at: formatDate(new Date(item.created)),
    updated_at: formatDate(new Date(item.lastModified)),
    product: item.cpeName.split(":")[4],
    version: item.cpeName.split(":")[5],
    cpe: item.cpeName
  }

    return (cpe);
}

const EARLIEST_DATE = new Date("01-01-1980");

export class CPEIngester {
  protected batcher: SimpleBatcher<CPE>;
  protected persistor: Persistor<CPE, { id: string }>;
  protected readonly progressHandler: ProgressHandler<Date>;
  protected pool: PoolExecutor;
  protected updatePersistor: Persistor<Update, string>;
  constructor(
    persistor: Persistor<CPE, { id: string }>,
    updatePersistor: Persistor<Update, string>,
  ) {
    this.pool = new PoolExecutor(5);
    this.batcher = new SimpleBatcher<CPE>(5000, this.flusher, this.pool);
    this.persistor = persistor;
    this.updatePersistor = updatePersistor;
  }
  populateCpes = async (props?: UpdateProps) => {
    const counter = new Counter();
    const upTo = new Date();
    // We get the date of last update from the database 
    const dbLastUpdate = await this.updatePersistor.findOne('CPE');
    console.log(dbLastUpdate?.last_update);
    console.log(`Last CPE update done : ${dbLastUpdate?.last_update}`);
    // Avant C'était 2, avec 1, je rends ça séquentiel, plus simple à traiter
    // Il faudrait traiter en mode séquentiel, donc plus besoin de pool, ni de batcher
    const pool = new PoolExecutor(1);
    const start = new Date().getTime();
    const batcher = new SimpleBatcher(2000, this.flusher, pool);
    // Si on n'a pas d'entrée pour la dernière maj, on fait du 01/01/1900 à upTo
    for await (const r of getCpesBetween( !(dbLastUpdate), dbLastUpdate ? dbLastUpdate.last_update : EARLIEST_DATE, upTo,)) {
      // ça c'est avec le pool, je veux le faire séquentiellement maintenant
      pool.submit(() =>
        doOnePage(r, counter, batcher)
      );

    }
    console.log("all download done, waiting for handlers to finish");
    await pool.isEmpty();
    // C'est cette fonction qui va faire les persist de tous les batchs à la fin.
    await batcher.flush();

    const end = new Date().getTime();
    const took = (end - start) / 1000;

    console.log(`done: ${counter.get()} in ${took} s`);
    console.log(`did ${counter.get() / took} items/s`);

    // On ne vérifie pas l'intégrité de chaque chunk ==> pourquoi si le code existe ?? 
    // await this.cveInfos.ensureChunksIntegrity(upTo); // checking that all the data does exist

    await this.updatePersistor.persistOne({
      id: 1,
      last_update: upTo,
      name: "CPE",
    });
  }
  /**
   * Called when a batcher is flushed with the batch being flushed
   * @param batch
   * @returns
   */
  protected flusher = async (batch: CPE[]) => {
    const size = batch.length;
    if (size === 0) {
      return;
    }
    console.log("doing batch");
    const start = new Date().getTime();
    const batchInsertResult = await this.persistor.persistMany(batch);

    const end = new Date().getTime();
    console.log("batch insert took", end - start, "for", batchInsertResult);
  };
}