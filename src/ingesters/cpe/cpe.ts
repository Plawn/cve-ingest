import { Readable } from "https://deno.land/std@0.177.0/node/stream.ts";
import { CPEFeed, CPEFeedType, RawCPE } from "./../../api/feed.ts";
import { Batcher, SimpleBatcher } from "./../../utils/batcher.ts";
import { prepareDate, sha1Hex, formatDate } from "./../../utils/format.ts";
import { tarGzStreamArrayExtractor } from "./../../utils/streaming.ts";
import { Persistor } from "./../../utils/persistor.ts";
import { CPE } from "./type.ts";
import { Counter, PoolExecutor, sleep, toMs } from "../../utils/pool.ts";
import { CpeChunk, CPENVD, Update, CPESearch, CPENVDResult } from "../../utils/types.ts";
import { getDateChunks } from "../../utils/utils.ts";
import { UpdateProps } from "../cve/type.ts";
import { ProgressHandler } from "../../utils/progress.ts";

/**
 * Taken from cve-search
 * @param cpe
 * @returns
 */
function generateTitle(cpe: string) {
  let title = "";

  const cpeSplit = cpe.split(":");
  // # Do a very basic test to see if the CPE is valid
  if (cpeSplit.length === 13) {
    // # Combine vendor, product and version
    title = cpeSplit.slice(3, 6).join(" ");

    // # If "other" is specified, add it to the title
    if (cpeSplit[12] != "*") {
      title += cpeSplit[12];
    }

    // # Capitilize each word
    // title = title.title()

    // # If the target_sw is defined, add "for <target_sw>" to title
    if (cpeSplit[10] != "*") {
      title += " for " + cpeSplit[10];
    }

    // # In CPE 2.3 spaces are replaced with underscores. Undo it
    title = title.replace("_", " ");

    // # Special characters are escaped with \. Undo it
    title = title.replace("\\", "");
  }

  return title;
}

// #####################################    Modification pour passer sur le feed CPE et non l'archive      ######################################### 

const pageSize = 2000;

async function getCpesBetweenPaged(
  start: Date,
  end: Date,
  page: number,
  pageSize: number,
// TODO : Décrire le CPENVDResult en se basant sur CVENVDResult
): Promise<CPENVDResult> {
  const startString = prepareDate(start);
  const endString = prepareDate(end);
  const startIndex = page * pageSize;
  // Pour un premier test, on va juste aller chercher les linux 
  const url = 
    `https://services.nvd.nist.gov/rest/json/cpes/2.0?cpeMatchString=cpe:2.3:a:linux:`;
  
  // const url = `https://services.nvd.nist.gov/rest/json/cpes/2.0/?lastModStartDate=${startString}&lastModEndDate=${endString}&startIndex=${startIndex}`;
  console.log("using url", url);
  for (; ;) {
    const res = await fetch(url);
    try {
      const text = await res.text();
      if (res.status > 300) {
        console.log("warn: going too fast");
        await sleep(toMs(10_000));
      } else {
        try {
          const value = JSON.parse(text);
          return value;
        } catch (_e) {
          console.error(res.status, text);
          // throw e;
        }
      }
    } catch (e) {
      console.error(res.status, e);
    }
  }
}

// in Days
const timeChukSize = 30;

async function* getCpesBetween(
  baseStart: Date,
  baseEnd: Date,
): AsyncGenerator<CpeChunk> {
  for (const [start, end] of getDateChunks(baseStart, baseEnd, timeChukSize)) {
    console.log(`Doing CPEs between ${start} and ${end}`);
    let hasRemaining = true;
    let page = 0;

    while (hasRemaining) {
      const result = await getCpesBetweenPaged(start, end, page, pageSize);
      // if the result is exactly the size of the page then we waste one query
      hasRemaining = (result.products.length >= pageSize);
      const expectedPages = result.totalResults / pageSize;
      page++;
      yield { result, page, expectedPages, start, end, hasRemaining };
    }
    console.log("On sort de la boucle, on ne devrait pas revenir ici !")
  }
}


function getUpdatedCpesSince(start: Date, upTo: Date) {
  const cpes = getCpesBetween(start, upTo);
  return cpes;
}


const getGenerator = (since: Date, upTo: Date, props?: UpdateProps) => {
  if (props) {
    return getCpesBetween(props.startDate, props.endDate);
  } else {
    return getUpdatedCpesSince(since, upTo);
  }
};


async function doOnePage(
  chunk: CpeChunk,
  counter: Counter,
  // pendingPages: Counter,
  batcher: Batcher<unknown>,
  progressHandler: ProgressHandler<Date>,
) {
  const {
    result,
    page,
    start: chunkSart,
    end: chunkEnd,
    expectedPages,
    hasRemaining,
  } = chunk;
  console.log("doing page", page, "of expected pages", expectedPages);
  const start = new Date().getTime();
  // add streaming here maybe
  const re = result.products
    .map((c) =>
      handleOne(c.cpe)
        .then((e) => {
          batcher.enqueue(e);
          counter.increment();
        })
    );
  // TODO Comprendre ce que ça fait vraiment ça. On attend que toutes les nouvelles entrées soient terminées d'être traitées là c'est ça ?
  await Promise.all(re);
  const end = new Date().getTime();
  const took = (end - start) / 1000;
  console.log(`page ${page} took ${took} s`);
  // ça je me dis que le garder peut être  intéressant si le script crash au milieu d'une exec, on peut repartir propre
  // progressHandler.addChunk({
  //   itemCount: re.length,
  //   start: chunkSart,
  //   end: chunkEnd,
  //   done: !hasRemaining,
  //   count: page,
  //   getBucketId: () => `${chunkSart}${chunkEnd}`,
  // });
}


async function handleOne(
  item: CPENVD
  ): Promise<CPESearch> {

  // TODO Faire un makeBase pour CPE ?? Ou alors on le fait direct ici ? 
  const cpe = {
    id: item.cpeNameId,
    created_at: new Date(item.created),
    updated_at: new Date(item.lastModified),
    product: item.cpeName.split(":")[4],
    version: item.cpeName.split(":")[5],
    cpe: item.cpeName
  }

    return (cpe);
}

// #################################       Fin Modification           ########################################################



/**
 * This function enables us to stream download the archive, while reading the content, while parsing the content, while pushing the result to the persistor
 *
 * This enables us to limit the memory consumption and do all the work in memory as a stream
 * This is the most efficient way to do it
 * @param stream
 * @param onItem
 * @returns
 */
function executePipeline(
  stream: Readable,
  onItem: (item: RawCPE) => Promise<void>,
  _pool: PoolExecutor,
) {
  // const pipeline = stream // here we have a stream of bytes
  //   .pipe(zlib.createGunzip()) // take the tar.gz archive and stream the content of the archive
  //   .pipe(parser()) // stream the bytes of the first file into a parser
  //   .pipe(pick({ filter: "matches" })) // takes the entry called "matches"
  //   .pipe(streamArray()); // takes the entries of "matches" and stream each token

  const pipeline = tarGzStreamArrayExtractor<CPEFeedType>(stream, "matches");
  let counter = 0;
  pipeline.on("data", ({ value }: { value: RawCPE }) => { // here we are called with each RawCPE item
    onItem(value).then(() => {
      ++counter;
      // console.log('counter');
      // console.log(counter);
      if (counter % 1000 === 0) {
        console.log(counter);
      }
    });
  });

  // just iterate
  // for await (const _ of pipeline) {
  //   if (pool.isPending()) {
  //     await sleep(50); // let time for background data push
  //   } // poors man's backpressure
  // }

  return new Promise<void>((resolve) => {
    pipeline.on("end", () => {
      // console.log('end');
      resolve();
    });
  });
}

function makeCpeAndVersion(item: RawCPE): [CPE, string] {
  const uri = item.cpe23Uri;
  const cpe: CPE = {
    title: generateTitle(uri),
    cpe_2_2: uri,
    cpe_name: item["cpe_name"],
    vendor: uri.split(":")[3],
    product: uri.split(":")[4],
    hash: "",
  };

  // could use a reduce if we want to get fancy
  let version_info = "";
  if ("versionStartExcluding" in item) {
    cpe.versionStartExcluding = item["versionStartExcluding"];
    version_info += cpe.versionStartExcluding + "_VSE";
  }

  if ("versionStartIncluding" in item) {
    cpe.versionStartIncluding = item["versionStartIncluding"];
    version_info += cpe.versionStartIncluding + "_VSI";
  }

  if ("versionEndExcluding" in item) {
    cpe.versionEndExcluding = item["versionEndExcluding"];
    version_info += cpe.versionEndExcluding + "_VEE";
  }

  if ("versionEndIncluding" in item) {
    cpe.versionEndIncluding = item["versionEndIncluding"];
    version_info += cpe.versionEndIncluding + "_VEI";
  }

  return [cpe, version_info];
}

function prepareCpeItem(item: RawCPE) {
  const [cpe, version_info] = makeCpeAndVersion(item);
  const sha1Hash = sha1Hex(cpe.cpe_2_2 + version_info);
  cpe.id = sha1Hash;
  return cpe;
}

export class CPEIngester {
  protected batcher: SimpleBatcher<CPE>;
  protected persistor: Persistor<CPE, { id: string }>;
  protected readonly progressHandler: ProgressHandler<Date>;
  protected pool: PoolExecutor;
  protected updatePersistor: Persistor<Update, { id: string }>;
  constructor(
    persistor: Persistor<CPE, { id: string }>,
    updatePersistor: Persistor<Update, { id: string }>,
  ) {
    this.pool = new PoolExecutor(1);
    this.batcher = new SimpleBatcher<CPE>(5000, this.flusher, this.pool);
    this.persistor = persistor;
    this.updatePersistor = updatePersistor;
  }
  populateCpes = async (props?: UpdateProps) => {
    //await this.cveInfos.ensureInfosExist(); // ensure collection exists in mongo

    const counter = new Counter();
    const upTo = new Date();
    // On récupère la date de dernière modification, l'id 1 pointant sur la date dernière modif des CPEs
    const dbLastUpdate = await this.updatePersistor.findOne(1);
    console.log(`Last update done : ${dbLastUpdate[0]}`);
    // Avant C'était 2, avec 1, je rends ça séquentiel, plus simple à traiter
    // Il faudrait traiter en mode séquentiel, donc plus besoin de pool, ni de batcher
    const pool = new PoolExecutor(1);
    const start = new Date().getTime();
    const batcher = new SimpleBatcher(1000, this.flusher, pool);
    // Si on n'a pas d'entrée pour la dernière maj, on fait du 01/01/1900 à upTo
    for await (const r of getGenerator(dbLastUpdate ? dbLastUpdate[0] : new Date("01-01-1900"), upTo)) {
      // ça c'est avec le pool, je veux le faire séquentiellement maintenant
      pool.submit(() =>
        doOnePage(r, counter, batcher, this.progressHandler)
      );

    }
    console.log("all download done, waiting for handlers to finish");
    await pool.isEmpty();
    // C'est cette fonction qui va faire les persist de tous les batchs à la fin.
    await batcher.flush();

    const end = new Date().getTime();
    const took = (end - start) / 1000;

    console.log(`done: ${counter.get()} in ${took} s`);
    console.log(`did ${counter.get() / took} items/s`);

    // On ne vérifie pas l'intégrité de chaque chunk ==> pourquoi si le code existe ?? 
    // await this.cveInfos.ensureChunksIntegrity(upTo); // checking that all the data does exist
    await this.updatePersistor.persistOne({
      id: 1,
      last_update: formatDate(upTo),
    });
  }
  /**
   * Called when a batcher is flushed with the batch being flushed
   * @param batch
   * @returns
   */
  protected 
  flusher = async (batch: CPE[]) => {
    const size = batch.length;
    if (size === 0) {
      return;
    }
    console.log("doing batch");
    const start = new Date().getTime();
    const batchInsertResult = await this.persistor.persistMany(batch);

    const end = new Date().getTime();
    console.log("batch insert took", end - start, "for", batchInsertResult);
  };

  public async update() {
    // TODO : Remplacer par CPEIngester nan ? 
    const f = new CPEFeed();
    //const dblastUpdate = await this.oldInfoPersistor.findOne({
      //db: this.dbName,
    //});
    //console.log("current data is", dblastUpdate?.["last-modified"]);
    const requiresUpdate = true;//await f.isUpToDate(
      //dblastUpdate?.["last-modified"],
    //);
    if (requiresUpdate) {
      console.log("not up to date, will download");
      const { stream: result, date } = await f.get(this.pool);
      console.log("download done");
      await this.upsertData(result);
      console.log("awaiting last batches");
      await this.batcher.flush();
      await this.updatePersistor.persistOne({
        id: "1",
        "last_update": date,
      });
    }
  }
}
